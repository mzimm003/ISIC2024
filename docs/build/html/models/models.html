<!DOCTYPE html>

<html lang="en" data-content_root="../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Models &#8212; ISIC 2024 0.0.1 documentation</title>
    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=d1102ebc" />
    <link rel="stylesheet" type="text/css" href="../_static/basic.css?v=c058f7c8" />
    <link rel="stylesheet" type="text/css" href="../_static/alabaster.css?v=e480df4b" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=cf12ce29" />
    <script src="../_static/documentation_options.js?v=d45e8c67"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Notes" href="../notes/notes.html" />
    <link rel="prev" title="Plan" href="../plan/plan.html" />
   
  <link rel="stylesheet" href="../_static/custom.css" type="text/css" />
  

  
  

  </head><body>
  <div class="document">
    
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="../index.html">ISIC 2024</a></h1>



<p class="blurb">Identify histologically confirmed skin cancer cases with single-lesion crops from 3D total body photos.</p>




<p>
<iframe src="https://ghbtns.com/github-btn.html?user=mzimm003&repo=https://github.com/mzimm003/ISIC2024&type=watch&count=true&size=large&v=2"
  allowtransparency="true" frameborder="0" scrolling="0" width="200px" height="35px"></iframe>
</p>






<search id="searchbox" style="display: none" role="search">
    <div class="searchformwrapper">
    <form class="search" action="../search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false" placeholder="Search"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script><h3>Navigation</h3>
<ul class="current">
<li class="toctree-l1"><a class="reference external" href="https://www.kaggle.com/competitions/isic-2024-challenge">Competition Site</a></li>
<li class="toctree-l1"><a class="reference internal" href="../plan/plan.html">Plan</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="../notes/notes.html">Notes</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
</ul>


<hr />
<ul>
    
    <li class="toctree-l1"><a href="https://mzimm003.github.io">Mark Zimmerman's Portfolio</a></li>
    
</ul>
<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="../index.html">Documentation overview</a><ul>
      <li>Previous: <a href="../plan/plan.html" title="previous chapter">Plan</a></li>
      <li>Next: <a href="../notes/notes.html" title="next chapter">Notes</a></li>
  </ul></li>
</ul>
</div>








        </div>
      </div>
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <section id="models">
<h1>Models<a class="headerlink" href="#models" title="Link to this heading">¶</a></h1>
<p>While the competition proceeds, I will attempt several iterations of a
classifier model. These will be a combination of my own ideas, research, and
ideas from the community Kaggle provides.</p>
<section id="preprocessing">
<h2>Preprocessing<a class="headerlink" href="#preprocessing" title="Link to this heading">¶</a></h2>
<blockquote>
<div><p>For each model I provide preprocessing options. Some preprocessing is
strictly necessary, but some make work in combination with certain models
better than others. How the preprocessing options are used will be specified
for each model. For the ISIC dataset, two major components exist. For each
skin lesion their is an image and a set of recorded features about the case.
Both must be preprocessed, but are done so distinctively.</p>
</div></blockquote>
<section id="images">
<h3>Images<a class="headerlink" href="#images" title="Link to this heading">¶</a></h3>
<blockquote>
<div><div class="twocol docutils container">
<div class="leftside docutils container">
<p><strong>Rescaling Color</strong> - Color in images is represented by 3 channels per
pixel. One each for red, green, and blue. The intensity of the color in
their respective channel is described on a range from 0 to 255. Machine
learning models often do better on smaller scales so it is important to be
able to redefine the intensity. In general, this process will linearly scale
the channels (e.g. map [0,255] → [0,1]).</p>
</div>
<div class="rightside docutils container">
<img alt="../_images/rescale_color.png" src="../_images/rescale_color.png" />
</div>
</div>
<div class="twocol docutils container">
<div class="leftside docutils container">
<p><strong>Padding</strong> - The images of the dataset are all different dimensions; varied
widths and heights. For most models, it is important they be consistent. One
way to accomplish this is simply expanding the smaller images to match the
largest one. Padding will add pixels around the edges of an image to create
the desired dimensions. The extra space can be filled in many ways, but for
our purposes the filled space will match the edge-most pixels of the
original image.</p>
</div>
<div class="rightside docutils container">
<img alt="../_images/padding.png" src="../_images/padding.png" />
</div>
</div>
<div class="twocol docutils container">
<div class="leftside docutils container">
<p><strong>Cropping</strong> - Another way to resize all images consistently, is to crop the
larger images to match the size of the smallest image. This has an added
benefit of augmenting our dataset in that most images will have multiple
representations, as a window smaller than the image is moved randomly about
to create the cropping.</p>
</div>
<div class="rightside docutils container">
<img alt="../_images/cropping.png" src="../_images/cropping.png" />
</div>
</div>
</div></blockquote>
</section>
<section id="features">
<h3>Features<a class="headerlink" href="#features" title="Link to this heading">¶</a></h3>
<blockquote>
<div><div class="twocol docutils container">
<div class="leftside docutils container">
<p><strong>Selection</strong> - Choosing which features are relevant and which might leak
target information to the model in training is important to the
generalization of the model. A few examples:</p>
<blockquote>
<div><ul class="simple">
<li><p>Features like ID tags can be excluded for being unique and arbitrary.</p></li>
<li><p>Features which all contain the same value can be can be excluded for
being uninformative.</p></li>
<li><p>Features which describe biopsy results can be excluded for leaking
target information (as a benign lesion would have no biopsy results).</p></li>
</ul>
</div></blockquote>
</div>
<div class="rightside docutils container">
<img alt="../_images/selection.png" src="../_images/selection.png" />
</div>
</div>
<div class="twocol docutils container">
<div class="leftside docutils container">
<p><strong>Ordinal Encoding</strong> - Some features are made up of various categories.
If these are described in text, it is difficult for many models to use.
Since numerical values are more easily understood, each category
within a feature a is assigned unique number, easily translating the
information while preserving the idea behind the categories.</p>
</div>
<div class="rightside docutils container">
<img alt="../_images/ordinal_encoding.png" src="../_images/ordinal_encoding.png" />
</div>
</div>
<div class="twocol docutils container">
<div class="leftside docutils container">
<p><strong>Fill NaN</strong> - When data is incomplete, values are generally still
expected by the models for every data point. So, a decision must be
made on how to fill in missing data. Average values from the data
points that are complete can be used, or just a value that would
otherwise never exist for that feature.</p>
</div>
<div class="rightside docutils container">
<img alt="../_images/fill_nan.png" src="../_images/fill_nan.png" />
</div>
</div>
</div></blockquote>
</section>
</section>
<section id="version-1-0">
<span id="v1-0"></span><h2>Version 1.0<a class="headerlink" href="#version-1-0" title="Link to this heading">¶</a></h2>
<blockquote>
<div><p>A vision transformer taking features as query tokens for the decoder. Image and
features are preprocessed, features are fed to a feature reducer, then all
combined by a transformer to produce a classification.</p>
</div></blockquote>
<section id="id1">
<h3>Preprocessing<a class="headerlink" href="#id1" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p><strong>Images</strong> - All image channels are linearly rescaled from [0,255] to [0,1].
In the case of the ISIC dataset, the smallest images forced a smaller cropping
of images than desired, so first images are padded to 200x200 (images larger
than this are unpadded), then all images are cropped to 125x125. The cropping
window positioning is selected randomly each time the image is loaded from the
dataset.</p>
<p><strong>Features</strong> -</p>
<blockquote>
<div><ul class="simple">
<li><p>Exclusions: Identification features are excluded from training data for being
irrelevant to diagnosis. These include <em>“isic_id”, “patient_id”, “lesion_id”,
“attribution”, “copyright_license”</em>. Further, features which exist only because
of a confirmed diagnosis are excluded, including <em>“iddx_full”, “iddx_1”,
“iddx_2”, “iddx_3”, “iddx_4”, “iddx_5”, “mel_mitotic_index”, “mel_thick_mm”,
“tbp_lv_dnn_lesion_confidence”</em>.</p></li>
<li><p>Ordinal Encoding: All text classifications which remain are assigned a
unique (within each feature) id number in place of the text description.</p></li>
<li><p>Fill NaN: Some <em>“age_approx”</em> values are missing, so these are filled as
-1 to help the model distinguish and lean less on this less distinctive
information.</p></li>
</ul>
</div></blockquote>
</div></blockquote>
</section>
<section id="model">
<h3>Model<a class="headerlink" href="#model" title="Link to this heading">¶</a></h3>
<blockquote>
<div><figure class="align-default" id="id12">
<span id="mod-arc"></span><img alt="../_images/model.png" src="../_images/model.png" />
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Basic flow of model architecture.</span><a class="headerlink" href="#id12" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p><a class="reference internal" href="#mod-arc"><span class="std std-numref">Fig. 1</span></a> shows how the information provided by the ISIC dataset is
processed. First, a feature reducer transforms the features which compliment the
images. This focuses the model on the most meaningful feature information
allowing for more effective use of the available data. In particular, for this
iteration of the model, Principal Component Analysis (PCA) is used including
enough dimensions to explain 99.99% of variance in the data.</p>
<p>Next, embeddings are created for both the image and the reduced feature set.
For the features, this is a small fully connected neural network; 2 layers with
a ReLU activation in between, the initial layer 64 nodes wide, the next twice
that, with the idea to create two 64 feature queries for the transformer
decoder. For the image, two embeddings are created. One, a patch embedding to
reduce the sequence length input into the transformer encoder, following the
idea of <span id="id2">Vaswani <em>et al.</em> [<a class="reference internal" href="../references.html#id4" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2023. URL: https://arxiv.org/abs/1706.03762, arXiv:1706.03762.">VSP+23</a>]</span>. Here, a patch of pixels
have their channel values concatenated, trading a greater number of features for
fewer transformer inputs. Further, a linear transformation is applied to allow
for varied patch sizes while maintaining a consistent feature dimension between
all embeddings. Two, a positional embedding is used to maintain information of
relative placement between patches. An embedding space of learnable parameters
is created the size of NxM, where N is the number of patches and M the desired
dimension of the features (again, 64 in this case).</p>
<p>The image embeddings, patch and positional, are then summed before taken as
input to the attention-based transformer encoder. The encoder has 4 layers of
attention with 8 heads, add and normalization, and 1024 dimension feed forward
networks (typical transformer encoder layers provided by
<span id="id3">Vaswani <em>et al.</em> [<a class="reference internal" href="../references.html#id4" title="Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. 2023. URL: https://arxiv.org/abs/1706.03762, arXiv:1706.03762.">VSP+23</a>]</span>). The result then used as memory in
conjunction with the queries created of the feature embeddings as input to the
decoder. The decoder is of similar dimension to the encoder.</p>
<p>Finally, the two queries create 2 sets of 1024 dimension outputs from the
transformer, which are flattened and passed to a linear layer to reduce all the
information down to logits representing whether the lesion is benign (dim 0) or
malignant (dim 1).</p>
</div></blockquote>
</section>
<section id="training">
<h3>Training<a class="headerlink" href="#training" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>To train the model first the feature reducer, PCA, is fit to the available
feature data. This process is quick and straightforward.</p>
<p>The trained feature reducer can then be used to feed the classifier model.
Training the classifier requires a balancing of the classifications. There
exist 400,666 benign lesions to 393 malignant, an imbalance which causes
little to be learned about malignant lesions. To address this, all available
malignant examples are duplicated within the dataset to create a roughly
equal number number of positive and negative classification examples.</p>
<p>With a balanced training set, a K-fold scheme is used to divide the dataset
into training and validation subsets. 4 folds were used in training.</p>
<p>An Adam based optimizer is used for its ability to achieve reasonable
results without significant effort put into tuning of hyperparameters.</p>
<p>To assess loss, cross entropy is used, taking the logits of the transformer
compared to the target provided by the data set.</p>
</div></blockquote>
</section>
<section id="results-pauc-0-021">
<h3>Results - pAUC: 0.021<a class="headerlink" href="#results-pauc-0-021" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>Over 4 iterations over the 4 folds, accuracy, precision, and recall end up
over 99%. However, once tested in competition, the score achieved is quite
poor, an pAUC of 0.021.</p>
</div></blockquote>
</section>
<section id="lessons-learned">
<h3>Lessons Learned<a class="headerlink" href="#lessons-learned" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>A poor score was expected, as multiple epochs have not yet been introduced
to the training regimen. However, given the training results compared to the
test, it is clear there is also a significant amount of information leakage.
Care must be taken in the balancing of the dataset so that some of the
malignant examples are held for the validation set and are in no way a part
of the training set. Further, it is important the K-fold process not use the
same model for different folds. Attention to these issues should make for a
better generalizing model.</p>
<p>Weights in the loss function may also help better balance the dataset and
enable better generalization, but will come at a cost of requiring many
training epochs.</p>
</div></blockquote>
</section>
</section>
<section id="version-1-1">
<span id="v1-1"></span><h2>Version 1.1<a class="headerlink" href="#version-1-1" title="Link to this heading">¶</a></h2>
<blockquote>
<div><p>Taking from the lessons learned in 1.0, corrections have been made to
prevent information leakage between training and validation data.
Additionally, scalable training has been introduced courtesy of the Ray
python library <span id="id4">[<a class="reference internal" href="../references.html#id5" title="Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: a distributed framework for emerging ai applications. 2018. URL: https://arxiv.org/abs/1712.05889, arXiv:1712.05889.">MNW+18</a>]</span>.</p>
</div></blockquote>
<section id="id5">
<h3>Training<a class="headerlink" href="#id5" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>To address the training set class imbalance, and to avoid data leakage,
weights have been applied to the cross entropy loss calculation. The impact
of loss calculated based on benign labels then is significantly less than
that of the malignant labels, with a weight of 393/401,059 to malignant’s
400,666/401,059. Given the very few number of malignant examples, it is
still expected many epochs will be necessary for good performance.</p>
<p>Further, the K-fold training scheme has been revised to create as many
models to train as their exist folds. However, for this training round in
particular, a single model is trained, and the dataset is simply split,
using 80% of it for training and the remaining 20% for validation, ensuring
a proportional number of classification examples in each split.</p>
<p>A future goal remains to balance the the dataset by duplicating malignant
examples. Each epoch will be more effective, and it affords the opportunity
to augment the dataset in other ways like various transformations of the
images.</p>
<p>With Ray <span id="id6">[<a class="reference internal" href="../references.html#id5" title="Philipp Moritz, Robert Nishihara, Stephanie Wang, Alexey Tumanov, Richard Liaw, Eric Liang, Melih Elibol, Zongheng Yang, William Paul, Michael I. Jordan, and Ion Stoica. Ray: a distributed framework for emerging ai applications. 2018. URL: https://arxiv.org/abs/1712.05889, arXiv:1712.05889.">MNW+18</a>]</span>, hardware
requirements can be defined per training instance. Then, depending on the
resources made available to the ray server, multiple training instances can
be run simultaneously, seen below in <a class="reference internal" href="#parallel-training"><span class="std std-numref">Fig. 2</span></a>.
Additionally, Ray provides a tuning module which allows an easy means of
exploring multiple training configurations, along with the application of
optimization algorithms. Specifically, for this version, 4 feature reduction
techniques are explored: none at all, and principal component analysis fit
to explain 80%, 99%, and 99.99% of data variance.</p>
<figure class="align-default" id="id13">
<span id="parallel-training"></span><img alt="../_images/parallel_training.png" src="../_images/parallel_training.png" />
<figcaption>
<p><span class="caption-number">Fig. 2 </span><span class="caption-text">Parallel training enabled by Ray library.</span><a class="headerlink" href="#id13" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>Each model was trained with only a difference in the dataset feature
reduction transformation and the following configuration in common:</p>
<blockquote>
<div><table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Option</p></th>
<th class="head"><p>Value</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Epochs</p></td>
<td><p>20</p></td>
</tr>
<tr class="row-odd"><td><p>Optimizer</p></td>
<td><p>Adam</p></td>
</tr>
<tr class="row-even"><td><p>Learning Rate</p></td>
<td><p>0.00005</p></td>
</tr>
</tbody>
</table>
</div></blockquote>
</div></blockquote>
</section>
<section id="results-pauc-0-109">
<h3>Results - pAUC: 0.109<a class="headerlink" href="#results-pauc-0-109" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>Much more promising than the previous version, validation results are now
much less than perfect, even after 5 times the training iterations. It seems
the data leakage problems have been addressed. Then, our validation results
are much more reliable in determining effective models.
<a class="reference internal" href="#tensorboard"><span class="std std-numref">Fig. 3</span></a> shows our slowly converging loss, true for both the
training and validation set, meaning learning is occurring and at least some
generalization of whats being learned can be expected. For this dataset,
because of the large proportion of benign results, ‘accuracy’ is poorly
representative of the models capability with respect to correctly
identifying malignant lesions. So as exciting as the greater than 99%
accuracy may be, more importantly to this case we track precision and
recall, focusing on the malignant examples.</p>
<figure class="align-default" id="id14">
<span id="tensorboard"></span><img alt="../_images/1.1tensorboard.png" src="../_images/1.1tensorboard.png" />
<figcaption>
<p><span class="caption-number">Fig. 3 </span><span class="caption-text">Training results for classifier trained with PCA explaining 99.99%
(00000), 99% (00001), 80% (00002) variance, and no feature reduction
(00003).</span><a class="headerlink" href="#id14" title="Link to this image">¶</a></p>
</figcaption>
</figure>
<p>While no result for recall or precision is great in
<a class="reference internal" href="#tensorboard"><span class="std std-numref">Fig. 3</span></a>, it is clear that no feature reduction, and PCA
capturing the most variance at 99.99% perform best. So, these are submitted
to the kaggle competition to ensure we are on the right track, with a marked
improvement in pAUC of .100 for the version using PCA, and .109 for the
version with no feature reduction at all.</p>
</div></blockquote>
</section>
<section id="id7">
<h3>Lessons Learned<a class="headerlink" href="#id7" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>Seen in <a class="reference internal" href="#tensorboard"><span class="std std-numref">Fig. 3</span></a>, precision and recall leave a lot to be
desired, yet it is clear the models are still learning from the continually
declining loss. Additional epochs will likely be of great use to the models,
though will cost significant time. A more complex learning rate scheme may
also be of use, scheduling a decaying rate for instance, enabling aggressive
learning up front, while still including nuanced learning capability toward
the end.</p>
</div></blockquote>
</section>
</section>
<section id="version-1-2">
<span id="v1-2"></span><h2>Version 1.2<a class="headerlink" href="#version-1-2" title="Link to this heading">¶</a></h2>
<blockquote>
<div><p>Recognizing 2 decoder feature queries as an arbitrary choice, I have
modified the sequence the decoder receives to be a set of queries, one for
each feature.</p>
</div></blockquote>
<section id="id8">
<h3>Model<a class="headerlink" href="#id8" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>The feature set, reduced or otherwise, is separated creating
a vector for each feature where all other features are zero. These vectors
are then fed to an embedding space, a dense neural network, which first
linearly projects each vector individually, passes that projection through
an activation function, then performs another linear projection
interconnecting the results. This provides the vectors a chance for
embedding considering only themselves as well as an embedding based on
relationships. These embedded feature vectors are then passed to the
decoder. The updated process is visualized in <a class="reference internal" href="#feature-embedding-update"><span class="std std-numref">Fig. 4</span></a>,
all other elements to the model remain the same.</p>
<figure class="align-default" id="id15">
<span id="feature-embedding-update"></span><img alt="../_images/feature_embedding_update.png" src="../_images/feature_embedding_update.png" />
<figcaption>
<p><span class="caption-number">Fig. 4 </span><span class="caption-text">Update to feature embedding process, including a mask to allow each
feature to produce a decoder query.</span><a class="headerlink" href="#id15" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</div></blockquote>
</section>
<section id="results-pauc-0-138">
<h3>Results - pAUC: 0.138<a class="headerlink" href="#results-pauc-0-138" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p><a class="reference internal" href="#feature-embedding-update"><span class="std std-numref">Fig. 4</span></a> shows a the new feature embedding scheme
much improved, particularly for the model including feature reduction, as
accuracy, precision, and recall all score higher. For the model without
feature reduction, precision scores as well as the previous models, lags in
recall, but seems to do considerably better in accuracy. The final test then
is a submission to the Kaggle competition, and the model with feature
reduction scores 0.131, while the model without scores 0.138.</p>
<figure class="align-default" id="id16">
<span id="id9"></span><img alt="../_images/1.2tensorboard.png" src="../_images/1.2tensorboard.png" />
<figcaption>
<p><span class="caption-number">Fig. 5 </span><span class="caption-text">Training results for classifier trained with old feature embedding
structure (d9499) and the new structure (91029), coupled with PCA
explaining 99.99% (00000) and no feature reduction (00003). Results have
been smoothed for clarity.</span><a class="headerlink" href="#id16" title="Link to this image">¶</a></p>
</figcaption>
</figure>
</div></blockquote>
</section>
<section id="id10">
<h3>Lessons Learned<a class="headerlink" href="#id10" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>The most obvious lesson is that the metrics currently used still make it
difficult to discern which model will perform best. Though more complicated,
a pAUC metric run during validation would be more relevant to the goals of
this competition.</p>
<p>Clearly the structure of the embeddings fed to the decoder is important. It
could be the increased length of the sequence that is more useful, the
holding features distinct in their embedding before relating them, or a
mix of both. Since the decoder already serves all kinds of inter-relational
analysis, it could serve to keep the features separate throughout the
embedding process, but the current process was chosen to save space.
An arbitrarily longer sequence could also be generated, which may be
worthwhile in the current structure where feature inter-relations are
embedded more than individual, so it could make sense to provide a sequence
with up to as many vectors as there are combinations of features.</p>
</div></blockquote>
</section>
</section>
<section id="version-1-3">
<span id="v1-3"></span><h2>Version 1.3<a class="headerlink" href="#version-1-3" title="Link to this heading">¶</a></h2>
<blockquote>
<div><p>To gain better generalization performance, I introduce greater randomization
in the preprocessing, particularly of the images.</p>
</div></blockquote>
<section id="id11">
<h3>Preprocessing<a class="headerlink" href="#id11" title="Link to this heading">¶</a></h3>
<blockquote>
<div><p>It can be difficult to anticipate the quality of the images to be received
in cases like different users submitting cell phone photos, the emulated
scenario for this competition. In all these cases of preprocessing, the aim
is to mitigate the impact of non-standard qualities which can be commonly
varied by smart phone filters and user indifference to orientation.</p>
<div class="twocol docutils container">
<div class="leftside docutils container">
<p><strong>Brightness Adjustment</strong> - A constant increase or decrease in all
pixel RGB values, capped at either side of the spectrum (white can’t
get whiter, black can’t get blacker).</p>
</div>
<div class="rightside docutils container">
<img alt="../_images/brightness_adjustment.png" src="../_images/brightness_adjustment.png" />
</div>
</div>
<div class="twocol docutils container">
<div class="leftside docutils container">
<p><strong>Contrast Adjustment</strong> - A multiplicative increase or decrease in
all pixel RGB values, capped at either side of the spectrum.</p>
</div>
<div class="rightside docutils container">
<img alt="../_images/contrast_adjustment.png" src="../_images/contrast_adjustment.png" />
</div>
</div>
<div class="twocol docutils container">
<div class="leftside docutils container">
<p><strong>Flip</strong> - An exchange of pixels, mirrored across a horizontal or
vertical axis, or both.</p>
</div>
<div class="rightside docutils container">
<img alt="../_images/flip_adjustment.png" src="../_images/flip_adjustment.png" />
</div>
</div>
</div></blockquote>
</section>
<section id="results-pauc-0-126">
<h3>Results - pAUC: 0.126<a class="headerlink" href="#results-pauc-0-126" title="Link to this heading">¶</a></h3>
</section>
</section>
<section id="version-1-4">
<h2>Version 1.4<a class="headerlink" href="#version-1-4" title="Link to this heading">¶</a></h2>
<blockquote>
<div><p>Greater number  of epochs.</p>
</div></blockquote>
<section id="results-pauc-0-149">
<h3>Results - pAUC: 0.149<a class="headerlink" href="#results-pauc-0-149" title="Link to this heading">¶</a></h3>
</section>
</section>
<section id="version-1-5">
<h2>Version 1.5<a class="headerlink" href="#version-1-5" title="Link to this heading">¶</a></h2>
<blockquote>
<div><p>Cyclical learning rate</p>
</div></blockquote>
</section>
</section>


          </div>
          
        </div>
      </div>
    <div class="clearer"></div>
  </div>
    <div class="footer">
      &#169;2024, Mark Zimmerman.
      
      |
      Powered by <a href="https://www.sphinx-doc.org/">Sphinx 8.0.2</a>
      &amp; <a href="https://alabaster.readthedocs.io">Alabaster 1.0.0</a>
      
      |
      <a href="../_sources/models/models.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>